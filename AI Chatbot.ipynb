{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: click in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (4.48.2)\n",
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tflearn) (1.18.5)\n",
      "Requirement already satisfied: six in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tflearn) (7.2.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py): started\n",
      "  Building wheel for tflearn (setup.py): finished with status 'done'\n",
      "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127304 sha256=8d6df54d2d300cf9c546bd420e96f8a0d6d6ebd3d44a33dfb7d30a22238506a6\n",
      "  Stored in directory: c:\\users\\ammuuu\\appdata\\local\\pip\\cache\\wheels\\5f\\14\\2e\\1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"intents.json\") as intents:\n",
    "    data = json.load(intents)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tag': 'greeting',\n",
       "  'patterns': ['Hi',\n",
       "   'How are you',\n",
       "   'Is anyone there?',\n",
       "   'Hello',\n",
       "   'Good day',\n",
       "   'Whats up'],\n",
       "  'responses': ['Hello!',\n",
       "   'Good to see you again!',\n",
       "   'Hi there, how can I help?'],\n",
       "  'context_set': ''},\n",
       " {'tag': 'goodbye',\n",
       "  'patterns': ['cya',\n",
       "   'See you later',\n",
       "   'Goodbye',\n",
       "   'I am Leaving',\n",
       "   'Have a Good day'],\n",
       "  'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'],\n",
       "  'context_set': ''},\n",
       " {'tag': 'age',\n",
       "  'patterns': ['how old',\n",
       "   'how old is tim',\n",
       "   'what is your age',\n",
       "   'how old are you',\n",
       "   'age?'],\n",
       "  'responses': ['I am 18 years old!', '18 years young!'],\n",
       "  'context_set': ''},\n",
       " {'tag': 'name',\n",
       "  'patterns': ['what is your name',\n",
       "   'what should I call you',\n",
       "   'whats your name?'],\n",
       "  'responses': ['You can call me Tim.',\n",
       "   \"I'm Tim!\",\n",
       "   \"I'm Tim aka Tech With Tim.\"],\n",
       "  'context_set': ''},\n",
       " {'tag': 'shop',\n",
       "  'patterns': ['Id like to buy something',\n",
       "   'whats on the menu',\n",
       "   'what do you reccommend?',\n",
       "   'could i get something to eat'],\n",
       "  'responses': ['We sell chocolate chip cookies for $2!',\n",
       "   'Cookies are on the menu!'],\n",
       "  'context_set': ''},\n",
       " {'tag': 'hours',\n",
       "  'patterns': ['when are you guys open',\n",
       "   'what are your hours',\n",
       "   'hours of operation'],\n",
       "  'responses': ['We are open 7am-4pm Monday-Friday!'],\n",
       "  'context_set': ''}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all words\n",
    "words = []\n",
    "#the possible labels\n",
    "labels = []\n",
    "#will hold the list of patterns\n",
    "docs_x = []\n",
    "#will hold the label for each pattern\n",
    "docs_y = []\n",
    "\n",
    "#loop through the intent dictionary\n",
    "# for intent in data['intents']:\n",
    "#     #loop through each pattern in each patterns list\n",
    "#     for pattern in intent['patterns']:\n",
    "#         #tokenize basically separates each word in a pattern and puts the words into a list\n",
    "#         wrds = nltk.word_tokenize(pattern)\n",
    "#         print(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up', 'cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day', 'how', 'old', 'how', 'old', 'is', 'tim', 'what', 'is', 'your', 'age', 'how', 'old', 'are', 'you', 'age', '?', 'what', 'is', 'your', 'name', 'what', 'should', 'I', 'call', 'you', 'whats', 'your', 'name', '?', 'Id', 'like', 'to', 'buy', 'something', 'whats', 'on', 'the', 'menu', 'what', 'do', 'you', 'reccommend', '?', 'could', 'i', 'get', 'something', 'to', 'eat', 'when', 'are', 'you', 'guys', 'open', 'what', 'are', 'your', 'hours', 'hours', 'of', 'operation']\n",
      "you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        #add all the wrds into words\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent['tag'])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "print(words)\n",
    "\n",
    "#cuts off the last letter of the word\n",
    "print(stemmer.stem(words[3]))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'how', 'ar', 'you', 'is', 'anyon', 'ther', 'hello', 'good', 'day', 'what', 'up', 'cya', 'see', 'you', 'lat', 'goodby', 'i', 'am', 'leav', 'hav', 'a', 'good', 'day', 'how', 'old', 'how', 'old', 'is', 'tim', 'what', 'is', 'yo', 'ag', 'how', 'old', 'ar', 'you', 'ag', 'what', 'is', 'yo', 'nam', 'what', 'should', 'i', 'cal', 'you', 'what', 'yo', 'nam', 'id', 'lik', 'to', 'buy', 'someth', 'what', 'on', 'the', 'menu', 'what', 'do', 'you', 'reccommend', 'could', 'i', 'get', 'someth', 'to', 'eat', 'when', 'ar', 'you', 'guy', 'op', 'what', 'ar', 'yo', 'hour', 'hour', 'of', 'op']\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "#removes the ends of the words\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplicates\n",
    "words = sorted(list(set(words)))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0]\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding\n",
    "#the mapping is [the, she, he, him, they, was, a, guy, person]\n",
    "#one-hot representation -> [0, 0, 1, 0, 0, 1, 1, 0, 1]\n",
    "#he was a person\n",
    "#this is a bag of words\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "#basically what we are doing here is one-hot encoding the data\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "# print(out_empty) -> [0,0,0,0,0,0]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "    \n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "    print(output_row)\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "    \n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "print()\n",
    "print(training)\n",
    "#the data is now ready to be used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 46)\n",
      "(26, 6)\n"
     ]
    }
   ],
   "source": [
    "#Reset any previous settings\n",
    "# tensorflow.reset_default_graph()\n",
    "\n",
    "#input data size\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "#hidden layers\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "#output layer, size equal to number of possibilities\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "#Regular deep neural network\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "print(training.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-d9df558f7306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[1;32m--> 184\u001b[1;33m                                       self.targets)\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[1;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# If a dict is provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
